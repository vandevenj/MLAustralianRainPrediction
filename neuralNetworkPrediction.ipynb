{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed6f47-d5fd-46da-8ff3-130ab2661f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e0f3c2c-3e27-441c-b024-a1e4bacb9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import trange\n",
    "\n",
    "%matplotlib inline\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 1234\n",
    "# cuDNN uses nondeterministic algorithms, set some options for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# read in dataset with date column parsed\n",
    "df = pd.read_csv('cleanedWeatherAUS.csv',\n",
    "    parse_dates=['Date'],\n",
    "    index_col='Date')\n",
    "\n",
    "\n",
    "y = df['RainTomorrow']\n",
    "xs = df[df.columns.difference(['RainTomorrow'])]\n",
    "\n",
    "# split dataset, 80% train 20% test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data_x, test_data_x, train_data_y, test_data_y = train_test_split(xs, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "74c6a97e-0254-4680-9879-c42494ff4d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "train_data = data_utils.TensorDataset(torch.Tensor(np.array(train_data_x)), torch.Tensor(np.array(train_data_y)))\n",
    "test_data = data_utils.TensorDataset(torch.Tensor(np.array(test_data_x)), torch.Tensor(np.array(test_data_y)))\n",
    "\n",
    "\n",
    "# Initial transform (convert to PyTorch Tensor only)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "## Use the following lines to check the basic statistics of this dataset\n",
    "# Calculate training data mean and standard deviation to apply normalization to data\n",
    "# train_data.data are of type uint8 (range 0,255) so divide by 255.\n",
    "# train_mean = train_data.data.double().mean() / 255.\n",
    "# train_std = train_data.data.double().std() / 255.\n",
    "# print(f'Train Data: Mean={train_mean}, Std={train_std}')\n",
    "\n",
    "## Optional: Perform normalization of train and test data using calculated training mean and standard deviation\n",
    "# This will convert data to be approximately standard normal\n",
    "# transform = transforms.Compose([\n",
    "#    transforms.ToTensor(),\n",
    "#    transforms.Normalize((train_mean, ), (train_std, ))\n",
    "# ])\n",
    "\n",
    "train_data.transform = transform\n",
    "test_data.transform = transform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "torch.manual_seed(seed)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=True) \n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ce1e0c52-4767-4716-ad2d-ab6c63385485",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)\n",
    "xs, labels = next(train_iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6438a735-a0ce-4e05-be83-fa03bc490158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNClassifier(\n",
      "  (fc1): Linear(in_features=21, out_features=64, bias=True)\n",
      "  (act): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (log_softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# input_size = 1 * 28 * 28  # input spatial dimension of images\n",
    "input_size = 21\n",
    "hidden_size = 64 #128         # width of hidden layer\n",
    "output_size = 10          # number of output neurons\n",
    "\n",
    "\n",
    "import math \n",
    "\n",
    "class NNClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        # self.flatten = torch.nn.Flatten(start_dim=1)\n",
    "\n",
    "        # ------------------\n",
    "        # Write your implementation here.\n",
    "        # initialization -> define structure here, layers, weight is randomized\n",
    "\n",
    "        # layer 1 input = vector\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        # ,512)\n",
    "\n",
    "        # activation function - ReLU \n",
    "        self.act = torch.nn.ReLU()        \n",
    "        \n",
    "        # layer 2 output shape: (512, 10)?\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "            #512,10)\n",
    "        \n",
    "        # output\n",
    "        self.log_softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        # ------------------\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: Input image is of shape [batch_size, 1, 28, 28]\n",
    "        \n",
    "        # Need to flatten to [batch_size, 784] before feeding to fc1\n",
    "        # x = self.flatten(x)\n",
    "\n",
    "        # output after layer 1\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # apply activation function\n",
    "        x = self.act(x)\n",
    "        \n",
    "        # output after layer 2\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # output after applying softmax\n",
    "        return self.log_softmax(x)\n",
    "        # ------------------\n",
    "\n",
    "model = NNClassifier().to(DEVICE)\n",
    "\n",
    "# sanity check\n",
    "print(model)\n",
    "# print([i in enumerate(model.parameters())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ccf194b8-afe7-4464-ae5c-5ae80dc560f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, device, optimizer, log_interval, epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    counter = []\n",
    "    cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for i, (img, label) in enumerate(train_loader):\n",
    "        img, label = img.to(device), label.to(device).long()\n",
    "        \n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # run forward on image data \n",
    "        outputs = model.forward(img)\n",
    "\n",
    "        # compute loss\n",
    "        loss = cross_entropy_loss(outputs, label)\n",
    "        loss.backward()\n",
    "\n",
    "        # adjust learning weights\n",
    "        optimizer.step()\n",
    "               \n",
    "        # Record training loss every log_interval and keep counter of total training images seen\n",
    "        if (i+1) % log_interval == 0:\n",
    "            losses.append(loss.item())\n",
    "            counter.append(\n",
    "                (i * batch_size) + img.size(0) + epoch * len(train_loader.dataset))\n",
    "\n",
    "    return losses, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "01a107df-42af-4d6d-86ca-06ac5a1e07b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_epoch(test_loader, model, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    num_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (features, label) in enumerate(test_loader):\n",
    "            features, label = features.to(device), label.to(device)\n",
    "\n",
    "            # ------------------\n",
    "            # Write your implementation here.\n",
    "            \n",
    "            output = model.forward(features)\n",
    "            pred = output.argmax(dim=1) \n",
    "            for iii in range(0, len(pred)):\n",
    "                num_correct = num_correct + 1 if pred[iii] == label[iii] else num_correct + 0\n",
    "                test_loss = test_loss + 0 if pred[iii] == label[iii] else test_loss + 1\n",
    "\n",
    "            # ------------------\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    return test_loss, num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ffb8314e-2f13-4a4f-8ca8-ccf8344c905c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|█████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lr = 0.01\n",
    "max_epochs=1\n",
    "gamma = 0.95\n",
    "\n",
    "# Recording data\n",
    "log_interval = 100\n",
    "\n",
    "# Instantiate optimizer (model was created in previous cell)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_correct = []\n",
    "for epoch in trange(max_epochs, leave=True, desc='Epochs'):\n",
    "    train_loss, counter = train_one_epoch(train_loader, model, DEVICE, optimizer, log_interval, epoch)\n",
    "    test_loss, num_correct = test_one_epoch(test_loader, model, DEVICE)\n",
    "\n",
    "    # Record results\n",
    "    train_losses.extend(train_loss)\n",
    "    train_counter.extend(counter)\n",
    "    test_losses.append(test_loss)\n",
    "    test_correct.append(num_correct)\n",
    "\n",
    "print(f\"Test accuracy: {test_correct[-1]/len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7a0e0-968a-40c8-89b6-f4d17810f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Draw training loss curve\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(train_counter, train_losses, label='Train loss')\n",
    "plt.plot([i * len(train_loader.dataset) for i in range(1, max_epochs + 1)], \n",
    "         test_losses, label='Test loss', marker='o')\n",
    "plt.xlim(left=0)\n",
    "plt.ylim(bottom=0)\n",
    "plt.title('Loss curve', fontsize=24)\n",
    "plt.xlabel('Number of training examples seen', fontsize=16)\n",
    "plt.ylabel('NLL', fontsize=16)\n",
    "plt.legend(loc='upper right', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6abcfb0-e009-4fea-acba-a121e9ccb755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
